{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `comp-00`: Multivariate pattern analysis (MVPA)\n",
    "This demo introduces multivariate pattern analysis (MVPA) using one of the datasets that popularized the application of machine learning to fMRI. Specifically, we will use classification models (e.g. the support vector machine; SVM) to differentiate between spatially-distributed cortical response patterns corresponding to different object stimuli (e.g. faces, houses, etc). Rather than using a regression model to predict brain activity at each voxel from the stimulus/task (sometimes referred to as an \"encoding\" model), multivariate pattern classification models predict the stimulus or task from distributed patterns of brain activity (referred to as a \"decoding\" model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual object recognition dataset\n",
    "We'll demo multivariate pattern classification on the visual object recognition dataset from [Haxby et al., 2001](https://doi.org/10.1126/science.1063736). This study popularized machine learning in fMRI and spurred a debate about localized versus distributed representation in human brain activity. Participants were presented with images from 8 object categories (bottles, cats, chairs, faces, houses, scissors, scrambled images, and shoes) interspersed with periods of fixation (referred to as \"rest\" here). The TR in this study was 2.5 seconds. In a given run, a block of images from each of the 8 categories was presented one time. Each block was ~9 TRs long and contained multiple rapid presentations of images from a single category. A subject received 12 scanning runs. We'll focus on data from one subject for the purposes of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/cup/people/zj4962/neu502b/neu502b-2024/comp-00'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset created in /mnt/cup/people/zj4962/neu502b/nilearn-data/haxby2001\n",
      "\n",
      "Downloading data from https://www.nitrc.org/frs/download.php/7868/mask.nii.gz ...\n",
      "Downloading data from http://data.pymvpa.org/datasets/haxby2001/MD5SUMS ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ...done. (0 seconds, 0 min)\n",
      " ...done. (0 seconds, 0 min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://data.pymvpa.org/datasets/haxby2001/subj2-2010.01.14.tar.gz ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 272015360 of 291168628 bytes (93.4%,    0.2s remaining) ...done. (4 seconds, 0 min)\n",
      "Extracting data from /mnt/cup/people/zj4962/neu502b/nilearn-data/haxby2001/def37a305edfda829916fa14c9ea08f8/subj2-2010.01.14.tar.gz..... done.\n"
     ]
    }
   ],
   "source": [
    "# Download Haxby 2001 data\n",
    "from nilearn import datasets\n",
    "\n",
    "# If you're on your local machine, change this path\n",
    "data_dir = '/mnt/cup/people/zj4962/neu502b/nilearn-data'\n",
    "\n",
    "haxby_dataset = datasets.fetch_haxby(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to extract certain attributes of the dataset, namely: the stimulus labels and the run labels. We'll exclude the the fixation TRs (the \"rest\" labels) from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in session metadata as pandas DataFrame\n",
    "session = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n",
    "\n",
    "# Extract stimuli and run labels for this subject\n",
    "stimuli, runs = session['labels'].values, session['chunks'].values\n",
    "\n",
    "# Create a boolean array indexing TRs containing a stimulus (non-rest)\n",
    "task_trs = stimuli != 'rest'\n",
    "\n",
    "# Get list of unique stimulus categories (excluding rest)\n",
    "categories = [c for c in np.unique(stimuli) if c != 'rest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the functional data using `index_img` to select only the task TRs; i.e. excluding intervening fixation (`'rest'`) periods. Check and interpret the shapes of the `stimuli_task` and `runs_task` labels with respect to the functional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have the same number of TRs for data and labels:\n",
    "from nilearn.image import index_img\n",
    "func_file = haxby_dataset.func[0]\n",
    "func_task = index_img(func_file, task_trs)\n",
    "stimuli_task=stimuli[task_trs]\n",
    "runs_task = runs[task_trs]\n",
    "assert len(stimuli_task) == len(runs_task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NiftiMasker(mask_img=&#x27;/mnt/cup/people/zj4962/neu502b/nilearn-data/haxby2001/subj2/mask4_vt.nii.gz&#x27;,\n",
       "            standardize=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;NiftiMasker<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>NiftiMasker(mask_img=&#x27;/mnt/cup/people/zj4962/neu502b/nilearn-data/haxby2001/subj2/mask4_vt.nii.gz&#x27;,\n",
       "            standardize=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "NiftiMasker(mask_img='/mnt/cup/people/zj4962/neu502b/nilearn-data/haxby2001/subj2/mask4_vt.nii.gz',\n",
       "            standardize=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masker_vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than applying classification analysis to the whole brain, we'll focus on a specifically on ventral temporal (VT) cortex due to it's role in visual object and category representation. Create a `NiftiMasker` object for ventral temporal (VT) cortex for use later; set `standardize=True` in the NiftiMasker to ensure that masked time series are z-scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the VT mask file and creater masker:\n",
    "from nilearn.maskers import NiftiMasker\n",
    "mask_vt = haxby_dataset['mask_vt'][0]\n",
    "masker_vt = NiftiMasker(mask_img=mask_vt, standardize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation using cross-validation\n",
    "To evaluate the quality of our classification model, we'll use a procedure called cross-validation. In this procedure, we train the model on a subset our data, then test the trained model on a left-out subset of data. This results in a classification score per cross-validation fold. When running a within-subjects classification analysis, the most common approach is to use leave-one-run-out cross-validation. In the current example, the dataset has 12 independent scanning runs. In leave-one-run-out cross-validation, the model will be trained on each subset of 11 runs and tested on the left-out 12th run, resulting in 12 classification scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sklearn's LeaveOneGroupOut cross-validation\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrow a function from sklearn to visualize the cross-validation folds\n",
    "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "               c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
    "               c=group, marker='_', lw=lw, cmap=cmap_data)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class', 'group']\n",
    "    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(X)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/people/zj4962/miniconda3/envs/neu502b/lib/python3.11/site-packages/nilearn/image/resampling.py:492: UserWarning: The provided image has no sform in its header. Please check the provided file. Results may not be as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up a couple variables for plotting\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "stimuli_map = {c: i for i, c in enumerate(categories)}\n",
    "stimuli_int = [stimuli_map[s] for s in stimuli_task]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plot_cv_indices(cv, masker_vt.fit_transform(func_task),\n",
    "                stimuli_int, runs_task, ax, 12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model\n",
    "Next, we'll load in Nilearn's `Decoder`, which provides a shortcut interface to some common classification models. We'll start with a basic support vector classifier (`svc`; [Boser et al., 1992](https://doi.org/10.1145/130385.130401)). There are a variety of ways to evaluate classifier performance. We'll use classification accuracy because this is a standard in the field (although other performance metrics maybe better; e.g. `roc_au`). Intialize the decoder with the SVC classifier using our leave-one-run-out cross-validation scheme above, specifying the VT mask and `accuracy` to evaluate performance. Get the cross-validated accuracy scores for the trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 64, 64, 864)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/people/zj4962/miniconda3/envs/neu502b/lib/python3.11/site-packages/nilearn/image/resampling.py:492: UserWarning: The provided image has no sform in its header. Please check the provided file. Results may not be as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import nilearn's Decoder for SVM with cross-validation\n",
    "from nilearn.decoding import Decoder\n",
    "\n",
    "# Initialize Decoder with SVC, leave-one-out CV, and VT mask:\n",
    "decoder = Decoder(estimator='svc', standardize=True,cv=cv,mask = masker_vt,scoring=\"accuracy\")\n",
    "# Fitting the decoder on the data across all CV folds:\n",
    "decoder.fit(func_task,stimuli_task,groups = run_task)\n",
    "\n",
    "# Get scores for each class and CV fold:\n",
    "scores = decoder.cv_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the accuracy scores for each stumulus category below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle: 0.5543981481481481\n",
      "cat: 0.6643518518518517\n",
      "chair: 0.6307870370370371\n",
      "face: 0.6504629629629629\n",
      "house: 0.6967592592592592\n",
      "scissors: 0.699074074074074\n",
      "scrambledpix: 0.7164351851851851\n",
      "shoe: 0.6307870370370371\n"
     ]
    }
   ],
   "source": [
    "# Print classification accuracy for each class:\n",
    "for stimulus,score in scores.items():\n",
    "    print(stimulus+\": \"+str(np.mean(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using Nilearn's convenient `Decoder` function, we can also use classifiers directly from scikit-learn. For example, here we'll recreate a similar model using scikit-learn's `LinearSVC`. First, initalize the classifier. Then, use `cross_val_score` to fit and evaluate the classifier; print the resulting accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m svm\u001b[38;5;241m.\u001b[39mfit(X[train], stimuli_task[train])\n\u001b[1;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X[test])\n\u001b[0;32m---> 16\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(stimuli_task[test], y_pred)\n\u001b[1;32m     17\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Load in automated cross-validation and classifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "cv = LeaveOneGroupOut()\n",
    "svm = LinearSVC()\n",
    "# X = masker_vt.transform(func_task)\n",
    "\n",
    "svm = LinearSVC()\n",
    "X = masker_vt.transform(func_task)\n",
    "scores = []\n",
    "for train, test in cv.split(X, stimuli_task, groups=runs_task):\n",
    "    svm.fit(X[train], stimuli_task[train])\n",
    "    y_pred = svm.predict(X[test])\n",
    "    acc = accuracy_score(stimuli_task[test], y_pred)\n",
    "    scores.append(acc)\n",
    "    \n",
    "# # Initialize SVC:\n",
    "# svc_model = LinearSVC()\n",
    "# # Run SVC with CV on VT data using cross_val_score:\n",
    "# y_pred = cross_val_score(svc_model, \n",
    "#                 X=masker_vt.fit_transform(func_task), \n",
    "#                 y=stimuli_task, \n",
    "#                 cv = cv,\n",
    "#                 groups=runs_task, \n",
    "#                 scoring=\"accuracy\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59722222, 0.54166667, 0.65277778, 0.66666667, 0.77777778,\n",
       "       0.77777778, 0.61111111, 0.65277778, 0.69444444, 0.63888889,\n",
       "       0.65277778, 0.72222222])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect classifier performance in more detail by visualizing the confusion matrix. Common metrics for quantifying classifer performance (e.g. accuracy, precision, AUROC) are summarizations of different aspects of the confusion matrix. Use `cross_val_predict` to re-run the classifier and store the predictions, then use `confusion_matrix` and `ConfusionMatrixDisplay` to visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [12, 864]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Re-fit model to explicitly get predicted labels:\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m confusion \u001b[38;5;241m=\u001b[39m confusion_matrix(scores,stimuli_task)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create confusion matrix from true and predicted labels:\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ConfusionMatrixDisplay(confusion)\n",
      "File \u001b[0;32m~/miniconda3/envs/neu502b/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/neu502b/lib/python3.11/site-packages/sklearn/metrics/_classification.py:319\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m    225\u001b[0m     {\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/miniconda3/envs/neu502b/lib/python3.11/site-packages/sklearn/metrics/_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/neu502b/lib/python3.11/site-packages/sklearn/utils/validation.py:430\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    428\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    433\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [12, 864]"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Re-fit model to explicitly get predicted labels:\n",
    "cm = confusion_matrix(stimuli_task,y_pred)\n",
    "# Create confusion matrix from true and predicted labels:\n",
    "ConfusionMatrixDisplay(confusion)\n",
    "# Plot confusion matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How \"good\" is our classification accuracy? Classification accuracy should be evaluated with respect to \"chance\" accuracy; i.e. the expected classification accuracy if the stimulus labels were randomly asigned. In a dataset with balanced class frequencies (i.e. same number of samples per class), the chance accuracy is typically $1 / n$ where $n$ is the number of distinct classes. For the current dataset, the chance accuracy is $1/8 = .125$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomizing labels\n",
    "To reinforce our intuition about \"chance\" decoding accuracy, we can shuffle the stimulus labels and re-run the entire classification algorithm. Although too computationally demanding for this demo, we could repeat this randomization procedure many times (e.g. 1000 permutations) to construct a null distibution (under the null hypothesis that there is no systematic relationship between class labels and activity patterns). This would amount to a permutation test and allow us to derive a *p*-value corresponding to our classification score. Below, use `np.random.permutation` to shuffle the stimulus labels, then re-run the classifier with `cross_val_score` and interpret the resulting accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle labels prior to classification:\n",
    "shuffled_labels = np.random.permutation(stimuli_task)\n",
    "\n",
    "# Re-run classifier using cross_val_score:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the weight vectors\n",
    "Similarly to regression, training a classifier model yields coefficients (or \"weights\") assigned to each feature in the model. Unlike the GLM, in the decoding framework, the features of the model are voxels; thus, a weight is assigned to each voxel indicating it's importance for successful classification. We can visualize these weights on the brain. Note, however, that these weight vectors are more difficult to interpret than activation maps ([Haufe et al., 2014](https://doi.org/10.1016/j.neuroimage.2013.10.067)). Let's go back to the Nilearn's fitted `Decoder` object from a previous cell. Extract the `coef_img_` for faces and for houses and visualize them using `plot_stat_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weight vector for the face class:\n",
    "from nilearn.plotting import plot_stat_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the weight vector for the house class:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First-level GLM prior to classification\n",
    "In the previous analyses, we have fed each time point corresponding to a given stimulus class into the classifier. However, for some experimental designs, it may be more appropriate to first perform a GLM, then supply the classifier with the beta weights (i.e. coefficients) from the GLM. This reduces the number of samples (i.e. response patterns) for the classifier, but may result in cleaner samples. Here, we'll use Nilearn's `FirstLevelModel` to run a GLM separately for each run, then provide the resulting contrast maps to the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build first-level GLM for each run\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "\n",
    "tr = 2.5\n",
    "\n",
    "events = {}\n",
    "for run in np.unique(runs):\n",
    "    stimuli_run = stimuli[runs == run]\n",
    "    n_trs = len(stimuli_run)\n",
    "    onset = tr * np.arange(n_trs)\n",
    "    duration = np.full(n_trs, tr)\n",
    "    \n",
    "    events_all = pd.DataFrame(\n",
    "        {'onset': onset, 'trial_type': stimuli_run, 'duration': duration})\n",
    "    events[run] = events_all[events_all['trial_type'] != 'rest']\n",
    "\n",
    "glm = FirstLevelModel(t_r=tr, hrf_model='spm',\n",
    "                      mask_img=mask_vt,\n",
    "                      drift_model='cosine',\n",
    "                      high_pass=1/128,\n",
    "                      standardize=True,\n",
    "                      noise_model='ar1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and collect beta maps\n",
    "glm_maps = []\n",
    "glm_categories = []\n",
    "glm_runs = []\n",
    "\n",
    "for run in np.unique(runs):\n",
    "    func_run = index_img(func_file, runs == run)\n",
    "    glm.fit(func_run, events=events[run])\n",
    "    for category in categories:\n",
    "        glm_maps.append(glm.compute_contrast(category))\n",
    "        glm_categories.append(category)\n",
    "        glm_runs.append(run)\n",
    "    print(f\"Finished fitting GLM for run {run}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the GLM report\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.reporting import make_glm_report\n",
    "\n",
    "func_mean = mean_img(func_file)\n",
    "make_glm_report(glm,\n",
    "                contrasts=categories,\n",
    "                bg_img=func_mean,\n",
    "                threshold=0,\n",
    "                height_control=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-run the `Decoder` with the `glm_maps` produced above as input. Make sure to specify the `glm_categories` as your target variable, and specify `glm_runs` as the `groups` variable for leave-one-out cross-validation. How does the model perform on these GLM betas relative to the raw time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the decoder on the GLM data across runs:\n",
    "\n",
    "# Get scores for each class and CV fold:\n",
    "\n",
    "# Print classification accuracy for each class:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimization using grid search\n",
    "Most classification models have multiple hyperparameters that will affect performance; for example, in SVMs, the hyperparameter $C$ controls the width of the margin used when positioning the decision boundary. The best hyperparameter setting will vary from dataset to dataset. In order to chose the best hyperparameter(s) in an unbiased way, we can evaluate a variety of hyperarameter settings (referred to as a \"grid\") using cross-validation nested within each of our training samples. In our previous examples, this hyperparameter optimization was either performed under the hood, or the software's default hyperparameter was used. Here, we'll use a larger grid of settings for $C$ in hopes of improving classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid of C hyperparameter settings\n",
    "param_grid = {'C': np.logspace(-8, 2, 11)}\n",
    "print(f\"C hyperparamater grid:\\n {param_grid['C']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit classification model with hyperparameter grid\n",
    "decoder = Decoder(estimator='svc', cv=cv,\n",
    "                  mask=mask_vt, scoring='accuracy',\n",
    "                  param_grid=param_grid,\n",
    "                  standardize=False)\n",
    "decoder.fit(glm_maps, glm_categories, groups=glm_runs)\n",
    "\n",
    "# Get scores for each class and CV fold\n",
    "scores = decoder.cv_scores_\n",
    "\n",
    "# Print classification accuracy for each class\n",
    "for category in categories:\n",
    "    print(f\"Mean classification accuracy for {category} \"\n",
    "          f\"stimuli: {np.mean(scores[category]):.3f}\")\n",
    "print(\"Overall mean classification accuracy: \"\n",
    "      f\"{np.mean([scores[c] for c in categories]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting the local vs. distributed debate\n",
    "Finally, we can incorporate the functional localizer masks for cortical areas that are maximally responsive to faces and houses. Haxby and colleagues demonstrated that even cortical areas that prefer e.g. faces encode information about houses and other object categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each ROI and run decoding model\n",
    "roi_masks = {'VT':'mask_vt', 'face':'mask_face', 'house':'mask_house'}\n",
    "\n",
    "roi_accuracies = {}\n",
    "for roi in roi_masks:\n",
    "    mask_file = haxby_dataset[roi_masks[roi]][0]\n",
    "    decoder = Decoder(estimator='svc', cv=cv,\n",
    "                      mask=mask_file, scoring='accuracy')\n",
    "    decoder.fit(func_task, stimuli_task, groups=runs_task)\n",
    "    roi_accuracies[roi] = decoder.cv_scores_\n",
    "    print(f\"Finished decoding analysis for {roi} cortex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies for all three ROIs\n",
    "import seaborn as sns\n",
    "\n",
    "roi_long = {'ROI': [], 'category': [], 'accuracy': []}\n",
    "for roi in roi_accuracies:\n",
    "    for category in roi_accuracies[roi]:\n",
    "        for acc in roi_accuracies[roi][category]:\n",
    "            roi_long['ROI'].append(roi)\n",
    "            roi_long['category'].append(category)\n",
    "            roi_long['accuracy'].append(acc)\n",
    "roi_long = pd.DataFrame(roi_long)\n",
    "\n",
    "sns.barplot(data=roi_long, x='category', y='accuracy', hue='ROI')\n",
    "plt.axhline(.125, linestyle='--', color='white');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "* Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. In *Proceedings of the Fifth Annual Workshop on Computational Learning Theory* (pp. 144-152). https://doi.org/10.1145/130385.130401\n",
    "\n",
    "* Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J. D., Blankertz, B., & Bießmann, F. (2014). On the interpretation of weight vectors of linear models in multivariate neuroimaging. *NeuroImage*, *87*, 96-110. https://doi.org/10.1016/j.neuroimage.2013.10.067\n",
    "\n",
    "* Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., & Pietrini, P. (2001). Distributed and overlapping representations of faces and objects in ventral temporal cortex. *Science*, *293*(5539), 2425–2430. https://doi.org/10.1126/science.1063736"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
